{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_subjects = ['101', '102', '103', '104', '106', '107', '108', '110', '212', '213',\n",
    "            '214', '215', '216', '217', '218', '219', '320', '321', '322', '323',\n",
    "            '324', '325', '327', '328', '329', '330', '331', '333', '334', '335', \n",
    "            '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', \n",
    "            '346', '347', '348', '349', '350', '451', '452', '453', '455', '456',\n",
    "            '457', '458', '459', '460', '462', '463', '464', '465', '467', '468', \n",
    "            '469', '470', '502', '503', '571', '572', '573', '574', '575', '577', \n",
    "            '578', '579', '580', '581', '582', '584', '585', '586', '587', '588',\n",
    "            '589', '590', '591', '592', '593', '594', '595', '596', '597', '598',\n",
    "            '604', '605', '606', '607', '608', '609', '610', '611', '612', '613',\n",
    "            '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', \n",
    "            '624', '625', '626', '627', '628', '629', '630', '631', '633', '634']\n",
    "\n",
    "post_subjects = ['101', '102', '103', '104', '106', '107', '108', '110',\n",
    "                 '212', '214', '215', '216', '217', '218', '219', '320', '321', \n",
    "                 '323', '324', '325', '327', '328', '330', '331', '333', '334',\n",
    "                 '335', '336', '337', '338', '339', '340', '341', '342', '343', \n",
    "                 '344', '345', '346', '347', '348', '349', '350', '451', '453', \n",
    "                 '455', '458', '459', '460', '462', '463', '464', '465', '467',\n",
    "                 '468', '469', '470', '502', '503', '571', '572', '573', '574',\n",
    "                 '577', '578', '581', '582', '584', '586', '587', '588', '589', \n",
    "                 '591', '592', '593', '594', '595', '596', '597', '598', '604', \n",
    "                 '605', '606', '607', '608', '609', '610', '612', '613', '614', \n",
    "                 '615', '617', '618', '619', '620', '621', '622', '623', '624', \n",
    "                 '625', '626', '627', '629', '630', '631', '633', '634']\n",
    "\n",
    "k = 0\n",
    "data_dir = '/Users/Katie/Dropbox/Data/anxiety_results'\n",
    "thresh = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global_meas = ['total positive', 'total negative', 'efficiency', 'path length', 'modularity']    \n",
    "mod = []\n",
    "pos = []\n",
    "neg = []\n",
    "eff = []\n",
    "pat = []\n",
    "cco = []\n",
    "\n",
    "for i in pre_subjects:\n",
    "    network = pd.read_csv(join(data_dir, '7-ntwk-soln', 'pre', i, '{0}_network_metrics.csv'.format(i)), \n",
    "                              sep = \",\", header=0, index_col=0)\n",
    "    network_wise = pd.read_csv(join(data_dir, '7-ntwk-soln', 'pre', i, '{0}_network_wise_metrics.csv'.format(i)),\n",
    "                               sep = \",\", header=0, index_col=0)\n",
    "    #network.plot()\n",
    "    #fig = plt.gcf()\n",
    "    #fig.savefig('/Users/Katie/Dropbox/Data/anxiety_results/7-ntwk-soln/{0}/{0}_network_metrics.png'.format(i))\n",
    "    mod.append(np.trapz(network['modularity'], dx=0.1))\n",
    "    pos.append(np.trapz(network['total positive'], dx=0.1))\n",
    "    neg.append(np.trapz(network['total negative'], dx=0.1))\n",
    "    eff.append(np.trapz(network['efficiency'], dx=0.1))\n",
    "    pat.append(np.trapz(network['path length'], dx=0.1))\n",
    "    avgs = []\n",
    "    for i in thresh:\n",
    "        means = []\n",
    "        for j in np.arange(0, 7):\n",
    "            means.append(float(network_wise['clustering coefficient'][i][1:-1].split()[j]))\n",
    "        avgs.append(np.average(means))\n",
    "    cco.append(np.trapz(avgs, dx=0.1))\n",
    "ntwk_global = {'Modularity': mod,\n",
    "               'Efficiency': eff,\n",
    "               'Path Length': pat,\n",
    "               'Total Positive Weight': pos,\n",
    "               'Total Negative Weight': neg, \n",
    "               'Clustering Coefficient': cco}\n",
    "ntwk_global_df = pd.DataFrame(ntwk_global, index=pre_subjects)\n",
    "ntwk_global_df.to_csv(join(data_dir, '7-ntwk-soln','pre-network-global-metrics.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_meas = ['total positive', 'total negative', 'efficiency', 'path length', 'modularity']    \n",
    "mod = []\n",
    "pos = []\n",
    "neg = []\n",
    "eff = []\n",
    "pat = []\n",
    "cco = []\n",
    "\n",
    "for subj in post_subjects:\n",
    "    network = pd.read_csv(join(data_dir, '7-ntwk-soln', 'post', subj, '{0}_network_metrics.csv'.format(subj)), \n",
    "                              sep = \",\", header=0, index_col=0)\n",
    "    network_wise = pd.read_csv(join(data_dir, '7-ntwk-soln', 'post', subj, '{0}_network_wise_metrics.csv'.format(subj)), \n",
    "                              sep = \",\", header=0, index_col=0)\n",
    "    mod.append(np.trapz(network['modularity']))\n",
    "    pos.append(np.trapz(network['total positive']))\n",
    "    neg.append(np.trapz(network['total negative']))\n",
    "    eff.append(np.trapz(network['efficiency']))\n",
    "    pat.append(np.trapz(network['path length']))\n",
    "    avgs = []\n",
    "    for i in thresh:\n",
    "        means = []\n",
    "        for j in np.arange(0, 7):\n",
    "            means.append(float(network_wise['clustering coefficient'][i][1:-1].split()[j]))\n",
    "        avgs.append(np.average(means))\n",
    "    cco.append(np.trapz(avgs, dx=0.1))\n",
    "ntwk_global = {'Modularity': mod,\n",
    "               'Efficiency': eff,\n",
    "               'Path Length': pat,\n",
    "               'Total Positive Weight': pos,\n",
    "               'Total Negative Weight': neg, \n",
    "               'Clustering Coefficient': cco}\n",
    "ntwk_global_df = pd.DataFrame(ntwk_global, index=post_subjects)\n",
    "ntwk_global_df.to_csv(join(data_dir, '7-ntwk-soln','post-network-global-metrics.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>total positive</th>\n",
       "      <th>total negative</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>path length</th>\n",
       "      <th>modularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.890409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056257</td>\n",
       "      <td>0.045010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>3.650904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145176</td>\n",
       "      <td>0.086926</td>\n",
       "      <td>0.209391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>4.963367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196122</td>\n",
       "      <td>0.118175</td>\n",
       "      <td>0.190711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>5.779114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202320</td>\n",
       "      <td>0.137598</td>\n",
       "      <td>0.119575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>6.182932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207895</td>\n",
       "      <td>0.147213</td>\n",
       "      <td>0.096294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>6.182932</td>\n",
       "      <td>-0.185774</td>\n",
       "      <td>0.034452</td>\n",
       "      <td>0.142789</td>\n",
       "      <td>0.112060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>6.182932</td>\n",
       "      <td>-0.613560</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.132604</td>\n",
       "      <td>0.153911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>6.182932</td>\n",
       "      <td>-1.230411</td>\n",
       "      <td>-0.029023</td>\n",
       "      <td>0.117917</td>\n",
       "      <td>0.225222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>6.182932</td>\n",
       "      <td>-2.476358</td>\n",
       "      <td>-0.041726</td>\n",
       "      <td>0.088252</td>\n",
       "      <td>0.427903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  total positive  total negative  efficiency  path length  \\\n",
       "0         0.1        1.890409        0.000000    0.056257     0.045010   \n",
       "1         0.2        3.650904        0.000000    0.145176     0.086926   \n",
       "2         0.3        4.963367        0.000000    0.196122     0.118175   \n",
       "3         0.4        5.779114        0.000000    0.202320     0.137598   \n",
       "4         0.5        6.182932        0.000000    0.207895     0.147213   \n",
       "5         0.6        6.182932       -0.185774    0.034452     0.142789   \n",
       "6         0.7        6.182932       -0.613560    0.001973     0.132604   \n",
       "7         0.8        6.182932       -1.230411   -0.029023     0.117917   \n",
       "8         0.9        6.182932       -2.476358   -0.041726     0.088252   \n",
       "\n",
       "   modularity  \n",
       "0    0.000000  \n",
       "1    0.209391  \n",
       "2    0.190711  \n",
       "3    0.119575  \n",
       "4    0.096294  \n",
       "5    0.112060  \n",
       "6    0.153911  \n",
       "7    0.225222  \n",
       "8    0.427903  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ntwk_global['Modularity'])\n",
    "len(post_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mod = []\n",
    "pos = []\n",
    "neg = []\n",
    "eff = []\n",
    "pat = []\n",
    "\n",
    "for i in pre_subjects:\n",
    "    region = pd.read_csv(join(data_dir, '7-ntwk-soln', 'pre', i, '{0}_region_metrics.csv'.format(i)), \n",
    "                              sep = \",\", header=0)\n",
    "    mod.append(np.trapz(region['modularity']))\n",
    "    pos.append(np.trapz(region['total positive']))\n",
    "    neg.append(np.trapz(region['total negative']))\n",
    "    eff.append(np.trapz(region['efficiency']))\n",
    "    pat.append(np.trapz(region['path length']))\n",
    "regn_global = {'Modularity': mod,\n",
    "               'Efficiency': eff,\n",
    "               'Path Length': pat,\n",
    "               'Total Positive Weight': pos,\n",
    "               'Total Negative Weight': neg}\n",
    "regn_global_df = pd.DataFrame(regn_global, index=pre_subjects)\n",
    "regn_global_df.to_csv(join(data_dir, '7-ntwk-soln','pre-region-global-metrics.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(post_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def region_wise_in(subject, data_dir):\n",
    "    from os.path import join\n",
    "    data_file = join(data_dir, '7-ntwk-soln/', subject,\n",
    "                     '{0}_region_wise_metrics.csv'.format(subject))\n",
    "    df = pd.read_csv(data_file, sep=',', header=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def integrate_local(df, region, metrics):\n",
    "    output = {}\n",
    "    #for each metric\n",
    "    while k < len(metrics):\n",
    "        meas = []\n",
    "        #make an array of a region's measure at each threshold\n",
    "        for i in np.arange(0, 1, 0.1):\n",
    "            meas.append(df.iloc[i][metrics[k]][region])\n",
    "        integral = np.trapz(meas)\n",
    "        output[metrics[k]] = integral\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nodes! we want default mode network (6), salience/ventral attention network (3), and dorsal attention network (5)\n",
    "networks = ['dmn', 'sn', 'dan']\n",
    "network_idx = [6, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull fc measures for each pairwise relationship\n",
    "dmn_sn = []\n",
    "sn_dan = []\n",
    "dan_dmn = []\n",
    "for subject in pre_subjects:\n",
    "    fc_df = pd.read_csv(join(data_dir, '7-ntwk-soln', 'pre', subject, '{0}_network_corrmat_Yeo7.csv'.format(subject)), \n",
    "                        sep = \",\", header=None)\n",
    "    dmn_sn.append(fc_df[6][3])\n",
    "    sn_dan.append(fc_df[3][5])\n",
    "    dan_dmn.append(fc_df[5][6])\n",
    "ntwk_fc = {'DMN-SN': dmn_sn,\n",
    "           'SN-DAN': sn_dan,\n",
    "           'DAN-DMN': dan_dmn}\n",
    "ntwk_fc_df = pd.DataFrame(ntwk_fc, index=pre_subjects)\n",
    "ntwk_fc_df.to_csv(join(data_dir, '7-ntwk-soln','pre-ntwk-fc.csv'), sep=',')\n",
    "\n",
    "dmn_sn = []\n",
    "sn_dan = []\n",
    "dan_dmn = []\n",
    "for subject in post_subjects:\n",
    "    fc_df = pd.read_csv(join(data_dir, '7-ntwk-soln', 'pre', subject, '{0}_network_corrmat_Yeo7.csv'.format(subject)), \n",
    "                        sep = \",\", header=None)\n",
    "    dmn_sn.append(fc_df[6][3])\n",
    "    sn_dan.append(fc_df[3][5])\n",
    "    dan_dmn.append(fc_df[5][6])\n",
    "ntwk_fc = {'DMN-SN': dmn_sn,\n",
    "           'SN-DAN': sn_dan,\n",
    "           'DAN-DMN': dan_dmn}\n",
    "ntwk_fc_df = pd.DataFrame(ntwk_fc, index=post_subjects)\n",
    "ntwk_fc_df.to_csv(join(data_dir, '7-ntwk-soln','post-ntwk-fc.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region_idx = [32, 33, 34, 35, 36, 37, 38, 39, \n",
    "              8, 12, 15, \n",
    "              20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "regions = ['rstg', 'rsmg', 'rifg', 'mfg', 'rphip', 'prec', 'lphip', 'lstg',\n",
    "           'rains', 'acc', 'lains', \n",
    "           'ritg', 'ripl', 'rmfg', 'riains', 'pcc', 'sma', 'lpcc', 'lsfg', \n",
    "           'lmfg', 'lofc', 'llipl', 'litg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OLD!!! For integrating region-wise measures, which the new plan doesn't do!\n",
    "output = {}\n",
    "\n",
    "for subj in subjects:\n",
    "    df = region_wise_in(subj)\n",
    "\n",
    "    #how to get array for each region of each local metric's values across thresholds\n",
    "    index = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    lins = []\n",
    "    rins = []\n",
    "    bacc = []\n",
    "\n",
    "    #for each threshold, grab betweenness for each region for ONE subject\n",
    "\n",
    "    for i in index:\n",
    "        lins.append(float(df['betweenness'][i][4:-1].split()[20]))\n",
    "        print lins\n",
    "        rins.append(float(df['betweenness'][i][4:-1].split()[13]))\n",
    "        bacc.append(float(df['betweenness'][i][4:-1].split()[17]))\n",
    "    #then integrate for each metric-region for ONE subject\n",
    "    acc_btwn.append(np.trapz(bacc))    #this adds the ACC-betweenness measure for this subj to array\n",
    "    lins_btwn.append(np.trapz(lins))\n",
    "    rins_btwn.append(np.trapz(rins))\n",
    "\n",
    "\n",
    "    lins = []\n",
    "    rins = []\n",
    "    bacc = []\n",
    "\n",
    "    for i in index:\n",
    "        lins.append(float(df['degree'][i][2:-1].split()[20]))\n",
    "        rins.append(float(df['degree'][i][2:-1].split()[13]))\n",
    "        bacc.append(float(df['degree'][i][2:-1].split()[17]))\n",
    "    acc_deg.append(np.trapz(bacc))    #this adds the ACC-betweenness measure for this subj to array\n",
    "    lins_deg.append(np.trapz(lins))\n",
    "    rins_deg.append(np.trapz(rins))\n",
    "    pos_weight = {}\n",
    "\n",
    "    lins = []\n",
    "    rins = []\n",
    "    bacc = []\n",
    "\n",
    "    for i in index:\n",
    "        lins.append(float(df['positive weights'][i][2:-1].split()[20]))\n",
    "        rins.append(float(df['positive weights'][i][2:-1].split()[13]))\n",
    "        bacc.append(float(df['positive weights'][i][2:-1].split()[17]))\n",
    "    acc_pos.append(np.trapz(bacc)) \n",
    "    lins_pos.append(np.trapz(lins))\n",
    "    rins_pos.append(np.trapz(rins))\n",
    "    \n",
    "    lins = []\n",
    "    rins = []\n",
    "    bacc = []\n",
    "\n",
    "    for i in index:\n",
    "        lins.append(float(df['negative weights'][i][2:-1].split()[20]))\n",
    "        rins.append(float(df['negative weights'][i][2:-1].split()[13]))\n",
    "        bacc.append(float(df['negative weights'][i][2:-1].split()[17]))\n",
    "    acc_neg.append(np.trapz(bacc)) \n",
    "    lins_neg.append(np.trapz(lins))\n",
    "    rins_neg.append(np.trapz(rins))\n",
    "    \n",
    "    lins = []\n",
    "    rins = []\n",
    "    bacc = []\n",
    "\n",
    "    for i in index:\n",
    "        lins.append(float(df['clustering coefficient'][i][2:-1].split()[20]))\n",
    "        rins.append(float(df['clustering coefficient'][i][2:-1].split()[13]))\n",
    "        bacc.append(float(df['clustering coefficient'][i][2:-1].split()[17]))\n",
    "    acc_cc.append(np.trapz(bacc)) \n",
    "    lins_cc.append(np.trapz(lins))\n",
    "    rins_cc.append(np.trapz(rins))\n",
    "output = {'ACC Betweenness': acc_btwn,'LInsula Betweenness': lins_btwn, 'RInsula Betweenness': rins_btwn,\n",
    "          'ACC Degree': acc_deg, 'LInsula Degree': lins_deg, 'RInsula Degree': rins_deg,\n",
    "          'ACC Positive Weight': acc_pos, 'LInsula Positive Weight': lins_pos, 'RInsula Positive Weight': rins_pos,\n",
    "          'ACC Negative Weight': acc_neg, 'LInsula Negative Weight': lins_neg, 'RInsula Negative Weight': rins_neg,\n",
    "          'ACC Clustering Coeff': acc_cc, 'LInsula Clustering Coeff': lins_cc, 'RInsula Clustering Coeff': rins_cc}\n",
    "output_df = pd.DataFrame(output, index=subjects)\n",
    "output_df.to_csv('/Users/Katie/Dropbox/Data/anxiety_results/7-ntwk-soln/region-local-metrics.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now do it all again for networks D:\n",
    "output = {}\n",
    "\n",
    "salience_btwn = []\n",
    "salience_deg = []\n",
    "salience_pos = []\n",
    "salience_neg = []\n",
    "salience_cc = []\n",
    "\n",
    "for subj in subjects:\n",
    "    df = region_wise_in(subj)\n",
    "\n",
    "    #how to get array for each region of each local metric's values across thresholds\n",
    "    index = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    #for each threshold, grab betweenness for each region for ONE subject\n",
    "    sal = []\n",
    "    for i in index:\n",
    "        #salience network is #4, but 3 with 0-indexing\n",
    "        sal.append(float(df['betweenness'][i][4:-1].split()[3]))\n",
    "    \n",
    "    #then integrate for each metric-region for ONE subject\n",
    "    salience_btwn.append(np.trapz(sal))    #this adds the salience-betweenness measure for this subj to array\n",
    "\n",
    "    sal = []\n",
    "\n",
    "    for i in index:\n",
    "        sal.append(float(df['degree'][i][2:-1].split()[3]))\n",
    "    salience_deg.append(np.trapz(sal))    #this adds the salience-degree measure for this subj to array\n",
    "\n",
    "    sal = []\n",
    "\n",
    "    for i in index:\n",
    "        sal.append(float(df['positive weights'][i][2:-1].split()[3]))\n",
    "    salience_pos.append(np.trapz(sal))\n",
    "    \n",
    "    sal = []\n",
    "\n",
    "    for i in index:\n",
    "        sal.append(float(df['negative weights'][i][2:-1].split()[3]))\n",
    "    salience_neg.append(np.trapz(sal)) \n",
    "    \n",
    "    sal = []\n",
    "\n",
    "    for i in index:\n",
    "        sal.append(float(df['clustering coefficient'][i][2:-1].split()[3]))\n",
    "        rins.append(float(df['clustering coefficient'][i][2:-1].split()[13]))\n",
    "        bacc.append(float(df['clustering coefficient'][i][2:-1].split()[17]))\n",
    "    salience_cc.append(np.trapz(sal)) \n",
    "\n",
    "output = {'Salience Betweenness': salience_btwn,\n",
    "          'Salience Degree': salience_deg, \n",
    "          'Salience Positive Weight': salience_pos, \n",
    "          'Salience Negative Weight': salience_neg, \n",
    "          'Salience Clustering Coeff': salience_cc}\n",
    "output_df = pd.DataFrame(output, index=subjects)\n",
    "output_df.to_csv('/Users/Katie/Dropbox/Data/anxiety_results/7-ntwk-soln/network-local-metrics.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I think dmn=6, sal=3, cen=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now do it all again for networks D:\n",
    "output = {}\n",
    "\n",
    "dmn_btwn = []\n",
    "dmn_deg = []\n",
    "dmn_pos = []\n",
    "dmn_neg = []\n",
    "dmn_cc = []\n",
    "\n",
    "for subj in subjects:\n",
    "    df = region_wise_in(subj)\n",
    "\n",
    "    #how to get array for each region of each local metric's values across thresholds\n",
    "    index = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    #for each threshold, grab betweenness for each region for ONE subject\n",
    "    dmn = []\n",
    "    for i in index:\n",
    "        #salience network is #4, but 3 with 0-indexing\n",
    "        dmn.append(float(df['betweenness'][i][4:-1].split()[5]))\n",
    "    \n",
    "    #then integrate for each metric-region for ONE subject\n",
    "    dmn_btwn.append(np.trapz(dmn))    #this adds the salience-betweenness measure for this subj to array\n",
    "\n",
    "    dmn = []\n",
    "\n",
    "    for i in index:\n",
    "        dmn.append(float(df['degree'][i][2:-1].split()[5]))\n",
    "    dmn_deg.append(np.trapz(dmn))    #this adds the salience-degree measure for this subj to array\n",
    "\n",
    "    dmn = []\n",
    "\n",
    "    for i in index:\n",
    "        dmn.append(float(df['positive weights'][i][2:-1].split()[5]))\n",
    "    dmn_pos.append(np.trapz(dmn))\n",
    "    \n",
    "    dmn = []\n",
    "\n",
    "    for i in index:\n",
    "        sal.append(float(df['negative weights'][i][2:-1].split()[5]))\n",
    "    dmn_neg.append(np.trapz(dmn)) \n",
    "    \n",
    "    dmn = []\n",
    "\n",
    "    for i in index:\n",
    "        dmn.append(float(df['clustering coefficient'][i][2:-1].split()[5]))\n",
    "    dmn_cc.append(np.trapz(dmn)) \n",
    "\n",
    "output = {'FPN Betweenness': dmn_btwn,\n",
    "          'FPN Degree': dmn_deg, \n",
    "          'FPN Positive Weight': dmn_pos, \n",
    "          'FPN Negative Weight': dmn_neg, \n",
    "          'FPN Clustering Coeff': dmn_cc}\n",
    "output_df = pd.DataFrame(output, index=subjects)\n",
    "output_df.to_csv('/Users/Katie/Dropbox/Data/anxiety_results/7-ntwk-soln/fpn-local-metrics.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmn_df = pd.read_csv('/Users/Katie/Dropbox/Data/anxiety_results/7-ntwk-soln/dmn-local-metrics.csv', sep=',', index_col=0)\n",
    "corrs = dmn_df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
